{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate File Finder\n",
    "See [README.md](README.md) for detailed documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mode ---\n",
    "# \"new\" = fresh scan, \"resume_scan\" = continue from checkpoint, \"load_report\" = skip to execution\n",
    "mode = \"new\"\n",
    "\n",
    "root_dir = \"C:/path/to/scan/\"          # used for \"new\" and \"resume_scan\"\n",
    "report_path = \"duplicate_report.txt\"   # used for \"load_report\", also where new reports are saved\n",
    "delete_mode = \"trash\"                  # \"trash\" or \"permanent\"\n",
    "default_keep_rule = \"oldest\"           # \"oldest\", \"newest\", \"shortest_path\", \"first_found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scan Filters (set to None to disable) ---\n",
    "ignore_extensions = None      # e.g. [\".jpg\", \".png\"]\n",
    "only_extensions = None        # e.g. [\".pdf\"] \u2014 mutually exclusive with ignore_extensions\n",
    "max_size = None               # e.g. 50\n",
    "max_size_unit = \"MB\"          # \"KB\", \"MB\", \"GB\"\n",
    "min_size = None               # e.g. 1\n",
    "min_size_unit = \"KB\"          # \"KB\", \"MB\", \"GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from src.duplicate_finder import (\n",
    "    ScanConfig, find_all_duplicate_files, generate_report,\n",
    "    clear_checkpoint, validate_checkpoint,\n",
    ")\n",
    "\n",
    "grouped = {}\n",
    "\n",
    "if mode in (\"new\", \"resume_scan\"):\n",
    "    if mode == \"new\":\n",
    "        clear_checkpoint(root_dir)\n",
    "    elif mode == \"resume_scan\":\n",
    "        assert validate_checkpoint(root_dir), \"No valid checkpoint found in \" + root_dir\n",
    "\n",
    "    config = ScanConfig(\n",
    "        root_dir=root_dir,\n",
    "        resume=(mode == \"resume_scan\"),\n",
    "        report_path=report_path,\n",
    "        ignore_extensions=ignore_extensions,\n",
    "        only_extensions=only_extensions,\n",
    "        max_size=max_size,\n",
    "        max_size_unit=max_size_unit,\n",
    "        min_size=min_size,\n",
    "        min_size_unit=min_size_unit,\n",
    "        default_keep_rule=default_keep_rule,\n",
    "    )\n",
    "\n",
    "    pbar = tqdm(desc=\"Scanning\", unit=\" files\")\n",
    "    grouped = find_all_duplicate_files(config, on_progress=lambda f: pbar.update(1))\n",
    "    pbar.close()\n",
    "\n",
    "    print(f'Scan complete. Found {len(grouped)} duplicate group(s).')\n",
    "else:\n",
    "    print('Mode is load_report \u2014 skipping scan. Run the report cells below.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grouped:\n",
    "    generate_report(grouped, report_path, keep_rule=default_keep_rule)\n",
    "\n",
    "    total_files = sum(len(files) for files in grouped.values())\n",
    "    to_remove_count = total_files - len(grouped)\n",
    "    removable_size = sum(\n",
    "        f['file_size'] for files in grouped.values() for f in files[1:]\n",
    "    )\n",
    "\n",
    "    def _fmt(size):\n",
    "        if size >= 1024**3: return f'{size/1024**3:.1f} GB'\n",
    "        if size >= 1024**2: return f'{size/1024**2:.1f} MB'\n",
    "        if size >= 1024:    return f'{size/1024:.1f} KB'\n",
    "        return f'{size} B'\n",
    "\n",
    "    print(f'Report written to: {report_path}')\n",
    "    print(f'{len(grouped)} groups, {to_remove_count} files to remove, {_fmt(removable_size)} recoverable')\n",
    "else:\n",
    "    print('No scan results. Using load_report mode or no duplicates found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the report file if needed. Each group must keep at least one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.duplicate_finder import load_report, validate_report, get_files_to_remove\n",
    "\n",
    "valid, message = validate_report(report_path)\n",
    "if not valid:\n",
    "    print(f'Report validation failed: {message}')\n",
    "else:\n",
    "    report = load_report(report_path)\n",
    "    to_remove = get_files_to_remove(report)\n",
    "    to_keep = len(report) - len(to_remove)\n",
    "    print(f'{to_keep} file(s) to keep, {len(to_remove)} file(s) to remove.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.duplicate_finder import remove_files, trash_files\n",
    "\n",
    "# Uncomment to execute:\n",
    "# if delete_mode == \"trash\":\n",
    "#     trash_files(to_remove)\n",
    "#     print(f'Moved {len(to_remove)} file(s) to trash.')\n",
    "# else:\n",
    "#     remove_files(to_remove)\n",
    "#     print(f'Permanently removed {len(to_remove)} file(s).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
